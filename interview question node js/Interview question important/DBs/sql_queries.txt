
-- rank with same ranking number skipping some rank
select "price", rank() over( order by "price" desc) as "Rank" from "Product"
-- rank with same ranking number without skipping some rank
select "price", dense_rank() over( order by "price" desc) as "Rank" from "Product"
-- rank without same rank
select "price", row_number() over( order by "price" desc) as "Rank" from "Product"

-- rank with same ranking number without skipping some rank and by category wise
select "category", "price", row_number() over( partition by "category" order by "price" desc) as "Rank" from "Product"


--rank 
select * from  (select "category", "price", row_number() over( order by "price" desc) as "Rank" from "Product") as "Rn" where "Rank"=2

--second largest salary
select max("price") from "Product" where "price" not in ( select max("price") from "Product") 

-- duplicat value
select "category", count(*) from "Product" group by "category" having count(*) = 2

-- name start with second letter L
select * where "Name" like '_L%'


category wise max salary
-select "category",(max(price) ) from "Product" group by "category" having "price" = max(price)
-select * from (select "category","price", row_number() over(partition by "category" order by "price" desc) from "Product") as c where "row_number"=1


How to handle concurrent request in postgres

Problem/Issues:- 
-dirty read
A transaction reads data written by a concurrent uncommitted transaction.

-nonrepeatable read
A transaction re-reads data it has previously read and finds that data has been modified by another transaction (that committed since the initial read).

-phantom read
A transaction re-executes a query returning a set of rows that satisfy a search condition and finds that the set of rows satisfying the condition has changed due to another recently-committed transaction.

-serialization anomaly
The result of successfully committing a group of transactions is inconsistent with all possible orderings of running those transactions one at a time.

To handle this there are 4 isolation level

Isolation_Level 	Dirty_Read	    Nonrepeatable_Read      Phantom_Read	Serialization_Anomaly
Read uncommitted	Allowed,nin PG	Possible	            Possible	Possible
Read committed	    Not possible	Possible	            Possible	Possible
Repeatable read	    Not possible	Not possible	        Allowed,nin PG	Possible
Serializable	    Not possible	Not possible	        Not possible	Not possible

Ref:- https://www.postgresql.org/docs/current/transaction-iso.html

-In postgres FOR UPDATE can be use at last of the update query to do the lock till update, other query will wait till updation is done .
-You can also use isolation level like IsolationLevel:"serializable" it also  do lock but will throw error if some one access it.
But both the method will have performance issue


In postgres optimization
-Prepared Statement: pre-gernerated statement for the query which is very frequently executed. But it work for same connection if conn closed you have to create again

PREPARE insert_task as INSERT INTO task (name,due_data,is_done) VALUES ($1,$2,$3);

-Using index , search will happen through b-tree
-Avoid lots of join
-Avoid using complex calculation at index query
-select some column instead of using *

-Indexing : for optimize scan

-Partitioning : segmented the data , larger table is converted into multi table based on some hased 
    CREATE TABLE emp () PARTITION BY RANGE (id) // create table by this way
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 1 TO 100 //create partitioned table by this way
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 100 TO 300
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 300 TO 900
-create Index idx on employee(department,last_name,first_name)
-Avoid using complex calculation in index column in where clause
-Use direct comparisons instead of wrapping the column in function
-Avoid sorting whole database , should use with limit

PostgreSQL processes more than 20 thousand transactions per second when MongoDB doesn't reach 2 thousand. PostgreSQL

When working with large numbers of client
connections, it is considered best practices to run PostgreSQL using a load balancer such
as PgBouncer for connection pooling.

Pg are much faster than mongo
Pg concurrent client could be more than 100
Mongo concurrent client could be defaults to 100

Transaction:- Is fundamental concept of ensuring data integrity and consistency
    - Single atomic of work the fully succeeds or fails

    ACID
    Atomicity:- If any part of trxn fails the entire trxn is rollback
    Consistency:- This means that any data written to the database must be valid according to all predefined rules and constraints.
    Isolation :- trxn operate independently of each other.
    Durability:- Once trxn committed , its changes are permanently stored in DB.

    Begin transaction,
    update employeeIncrement salary=10000 where empId=2;
    savepoint t1
    update employeeSalary salary=salary+10000 where empId=2;
    savepoint t2

    commit
    rollback to savepoint t1 :- to rollback to certain point
    release savepoint t1 :- to release till t1 represent that you are satisfied till that trxn

    insert into emp () value(),VAue(),value()

    Sometime table got locked when it stuck in some other trxn 
    To check the PID

    Select state,datname,pid,* from pg_stat_activity

    To delete the PID and unlock the table
    SELECT pg_terminate_backend(pid)
    FROM pg_stat_activity
    where datname='dbname' and pid=pid_number