
-- rank with same ranking number skipping some rank
select "price", rank() over( order by "price" desc) as "Rank" from "Product"
-- rank with same ranking number without skipping some rank
select "price", dense_rank() over( order by "price" desc) as "Rank" from "Product"
-- rank without same rank
select "price", row_number() over( order by "price" desc) as "Rank" from "Product"

-- rank with same ranking number without skipping some rank and by category wise
select "category", "price", row_number() over( partition by "category" order by "price" desc) as "Rank" from "Product"


--rank 
select * from  (select "category", "price", row_number() over( order by "price" desc) as "Rank" from "Product") as "Rn" where "Rank"=2

--second largest salary
select max("price") from "Product" where "price" not in ( select max("price") from "Product") 

-- duplicat value
select "category", count(*) from "Product" group by "category" having count(*) = 2

-- name start with second letter L
select * where "Name" like '_L%'


category wise max salary
-select "category",(max(price) ) from "Product" group by "category" having "price" = max(price)
-select * from (select "category","price", row_number() over(partition by "category" order by "price" desc) from "Product") as c where "row_number"=1


How to handle concurrent request in postgres

Problem/Issues:- 
-dirty read
A transaction reads data written by a concurrent uncommitted transaction.

-nonrepeatable read
A transaction re-reads data it has previously read and finds that data has been modified by another transaction (that committed since the initial read).

-phantom read
A transaction re-executes a query returning a set of rows that satisfy a search condition and finds that the set of rows satisfying the condition has changed due to another recently-committed transaction.

-serialization anomaly
The result of successfully committing a group of transactions is inconsistent with all possible orderings of running those transactions one at a time.

To handle this there are 4 isolation level

Isolation_Level 	Dirty_Read	    Nonrepeatable_Read      Phantom_Read	Serialization_Anomaly
Read uncommitted	Allowed,nin PG	Possible	            Possible	Possible
Read committed	    Not possible	Possible	            Possible	Possible
Repeatable read	    Not possible	Not possible	        Allowed,nin PG	Possible
Serializable	    Not possible	Not possible	        Not possible	Not possible

Ref:- https://www.postgresql.org/docs/current/transaction-iso.html


In postgres optimization
-Prepared Statement: pre-gernerated statement for the query which is very frequently executed. But it work for same connection if conn closed you have to create again

PREPARE insert_task as INSERT INTO task (name,due_data,is_done) VALUES ($1,$2,$3);

-Indexing : for optimize scan

-Partitioning : segmented the data , larger table is converted into multi table based on some hased 
    CREATE TABLE emp () PARTITION BY RANGE (id) // create table by this way
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 1 TO 100 //create partitioned table by this way
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 100 TO 300
    CREATE TABLE id_1_to_id_100 FOR VALUES FROM 300 TO 900
-create Index idx on employee(department,last_name,first_name)
-Avoid using complex calculation in index column in where clause
-Use direct comparisons instead of wrapping the column in function
-Avoid sorting whole database , should use with limit